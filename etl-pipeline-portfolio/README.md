# ğŸš€ ETL Pipeline End-to-End

[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/)
[![Apache Airflow](https://img.shields.io/badge/airflow-2.8+-green.svg)](https://airflow.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/postgresql-15+-blue.svg)](https://www.postgresql.org/)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://www.docker.com/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Pipeline ETL (Extract, Transform, Load) completo e escalÃ¡vel para processar dados de mÃºltiplas fontes, aplicar transformaÃ§Ãµes complexas e carregar em data warehouse. Projeto desenvolvido seguindo as melhores prÃ¡ticas da engenharia de dados.

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Arquitetura](#arquitetura)
- [Funcionalidades](#funcionalidades)
- [Tecnologias](#tecnologias)
- [InÃ­cio RÃ¡pido](#inÃ­cio-rÃ¡pido)
- [Estrutura do Projeto](#estrutura-do-projeto)
- [Casos de Uso](#casos-de-uso)

## ğŸ¯ VisÃ£o Geral

Este projeto demonstra um pipeline ETL completo que:

- **Extrai** dados de mÃºltiplas fontes (APIs, CSV, Bancos de Dados)
- **Transforma** dados aplicando limpeza, validaÃ§Ã£o e enriquecimento
- **Carrega** dados em PostgreSQL com estratÃ©gias incrementais e full-load
- **Orquestra** processos usando Apache Airflow
- **Monitora** execuÃ§Ã£o com logs detalhados e alertas
- **Testa** cÃ³digo com cobertura automatizada

### Fontes de Dados

1. **API REST** - Dados de clima (OpenWeatherMap)
2. **CSV Files** - Dados de vendas e transaÃ§Ãµes
3. **PostgreSQL** - Dados operacionais
4. **Web Scraping** - Dados pÃºblicos (opcional)

### Destino

- **PostgreSQL** - Data Warehouse dimensional

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FONTES DE DADOS                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   API REST  â”‚  CSV Files   â”‚  PostgreSQL  â”‚  Web Scraping   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚              â”‚                â”‚
       â–¼             â–¼              â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EXTRACT LAYER                           â”‚
â”‚  â€¢ API Connector   â€¢ File Reader   â€¢ DB Connector            â”‚
â”‚  â€¢ Error Handling  â€¢ Retry Logic   â€¢ Data Validation         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RAW DATA STORAGE                          â”‚
â”‚             (data/raw - Staging Area)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFORM LAYER                           â”‚
â”‚  â€¢ Data Cleaning    â€¢ Deduplication   â€¢ Type Conversion      â”‚
â”‚  â€¢ Validation       â€¢ Enrichment      â€¢ Business Rules       â”‚
â”‚  â€¢ Aggregation      â€¢ Feature Engineering                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PROCESSED DATA STORAGE                       â”‚
â”‚            (data/processed - Clean Data)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      LOAD LAYER                              â”‚
â”‚  â€¢ Incremental Load  â€¢ Full Refresh  â€¢ Upsert Strategy       â”‚
â”‚  â€¢ Data Quality Check â€¢ Transaction Management               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DATA WAREHOUSE                             â”‚
â”‚              PostgreSQL - Star Schema                        â”‚
â”‚    â€¢ Fact Tables    â€¢ Dimension Tables    â€¢ Indexes          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ORCHESTRATION & MONITORING                   â”‚
â”‚     Apache Airflow + Logging + Alertas                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## âœ¨ Funcionalidades

### Extract (ExtraÃ§Ã£o)

- âœ… **API REST Integration** - ExtraÃ§Ã£o de APIs com paginaÃ§Ã£o
- âœ… **File Processing** - Suporte para CSV, JSON, Parquet
- âœ… **Database Extraction** - Queries incrementais
- âœ… **Error Handling** - Retry logic e fallback
- âœ… **Rate Limiting** - Controle de requisiÃ§Ãµes

### Transform (TransformaÃ§Ã£o)

- âœ… **Data Cleaning** - RemoÃ§Ã£o de duplicatas, valores nulos
- âœ… **Type Conversion** - ConversÃ£o de tipos de dados
- âœ… **Validation** - Great Expectations para qualidade
- âœ… **Enrichment** - CÃ¡lculos derivados e joins
- âœ… **Aggregation** - MÃ©tricas e KPIs

### Load (Carga)

- âœ… **Incremental Load** - Apenas dados novos/modificados
- âœ… **Full Refresh** - Recarga completa quando necessÃ¡rio
- âœ… **Upsert Strategy** - Insert ou Update conforme necessÃ¡rio
- âœ… **Transaction Management** - ACID compliance
- âœ… **Performance Optimization** - Batch processing

### OrquestraÃ§Ã£o

- âœ… **Apache Airflow** - Agendamento e execuÃ§Ã£o
- âœ… **DAG Dependencies** - Gerenciamento de dependÃªncias
- âœ… **Retry Logic** - RecuperaÃ§Ã£o automÃ¡tica de falhas
- âœ… **Alertas** - NotificaÃ§Ãµes por email/Slack
- âœ… **Monitoring** - Dashboard de execuÃ§Ã£o

### Qualidade

- âœ… **Unit Tests** - Pytest para cÃ³digo
- âœ… **Integration Tests** - Testes end-to-end
- âœ… **Data Quality** - ValidaÃ§Ãµes automatizadas
- âœ… **Logging** - Logs detalhados de cada etapa
- âœ… **Documentation** - CÃ³digo auto-documentado

## ğŸ› ï¸ Tecnologias

### Core Stack

- **Python 3.11+** - Linguagem principal
- **Apache Airflow** - OrquestraÃ§Ã£o de workflows
- **PostgreSQL** - Data Warehouse
- **Pandas** - ManipulaÃ§Ã£o de dados
- **SQLAlchemy** - ORM para banco de dados

### Data Processing

- **Pandas** - DataFrames e anÃ¡lise
- **NumPy** - OperaÃ§Ãµes numÃ©ricas
- **Great Expectations** - ValidaÃ§Ã£o de qualidade

### Infrastructure

- **Docker** - ContainerizaÃ§Ã£o
- **Docker Compose** - OrquestraÃ§Ã£o local
- **GitHub Actions** - CI/CD

### Testing & Quality

- **Pytest** - Testes unitÃ¡rios
- **Pytest-cov** - Cobertura de cÃ³digo
- **Black** - FormataÃ§Ã£o de cÃ³digo
- **Flake8** - Linting
- **Pre-commit** - Hooks de commit

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos

- Docker & Docker Compose
- Python 3.11+
- Git

### InstalaÃ§Ã£o

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/seu-usuario/etl-pipeline-portfolio.git
cd etl-pipeline-portfolio

# 2. Configure o ambiente
cp .env.example .env
# Edite .env com suas credenciais

# 3. Inicie os containers
docker-compose up -d

# 4. Instale as dependÃªncias (se rodar local)
python -m venv venv
source venv/bin/activate  # Linux/Mac
pip install -r requirements.txt

# 5. Execute o pipeline
python src/main.py
# ou via Airflow: http://localhost:8080
```

### Acesso Ã s Interfaces

- **Airflow UI**: http://localhost:8080 (airflow/airflow)
- **PostgreSQL**: localhost:5432 (etl_user/etl_password)
- **Logs**: `data/logs/`

## ğŸ“ Estrutura do Projeto

```
etl-pipeline-portfolio/
â”‚
â”œâ”€â”€ src/                          # CÃ³digo fonte
â”‚   â”œâ”€â”€ extract/                  # MÃ³dulos de extraÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ api_extractor.py     # ExtraÃ§Ã£o de APIs
â”‚   â”‚   â”œâ”€â”€ csv_extractor.py     # ExtraÃ§Ã£o de CSVs
â”‚   â”‚   â””â”€â”€ db_extractor.py      # ExtraÃ§Ã£o de DBs
â”‚   â”‚
â”‚   â”œâ”€â”€ transform/                # MÃ³dulos de transformaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ cleaner.py           # Limpeza de dados
â”‚   â”‚   â”œâ”€â”€ validator.py         # ValidaÃ§Ã£o de qualidade
â”‚   â”‚   â””â”€â”€ aggregator.py        # AgregaÃ§Ãµes
â”‚   â”‚
â”‚   â”œâ”€â”€ load/                     # MÃ³dulos de carga
â”‚   â”‚   â”œâ”€â”€ db_loader.py         # Carga no PostgreSQL
â”‚   â”‚   â””â”€â”€ strategies.py        # EstratÃ©gias de carga
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/                    # UtilitÃ¡rios
â”‚   â”‚   â”œâ”€â”€ logger.py            # Sistema de logs
â”‚   â”‚   â”œâ”€â”€ config.py            # ConfiguraÃ§Ãµes
â”‚   â”‚   â””â”€â”€ db_connection.py    # ConexÃ£o com DB
â”‚   â”‚
â”‚   â””â”€â”€ main.py                   # Orquestrador principal
â”‚
â”œâ”€â”€ airflow/                      # Apache Airflow
â”‚   â””â”€â”€ dags/                     # DAGs
â”‚       â”œâ”€â”€ etl_daily.py         # Pipeline diÃ¡rio
â”‚       â””â”€â”€ etl_weekly.py        # Pipeline semanal
â”‚
â”œâ”€â”€ config/                       # Arquivos de configuraÃ§Ã£o
â”‚   â”œâ”€â”€ database.yaml            # Config de banco
â”‚   â””â”€â”€ pipeline.yaml            # Config do pipeline
â”‚
â”œâ”€â”€ data/                         # Dados
â”‚   â”œâ”€â”€ raw/                     # Dados brutos
â”‚   â”œâ”€â”€ processed/               # Dados processados
â”‚   â””â”€â”€ logs/                    # Logs de execuÃ§Ã£o
â”‚
â”œâ”€â”€ tests/                        # Testes
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”‚
â”œâ”€â”€ notebooks/                    # Jupyter Notebooks
â”‚   â””â”€â”€ exploratory_analysis.ipynb
â”‚
â”œâ”€â”€ docs/                         # DocumentaÃ§Ã£o
â”‚   â”œâ”€â”€ architecture.md
â”‚   â””â”€â”€ api_documentation.md
â”‚
â”œâ”€â”€ docker/                       # Dockerfiles
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ .github/workflows/            # CI/CD
â”‚   â””â”€â”€ ci.yml
â”‚
â”œâ”€â”€ docker-compose.yml            # OrquestraÃ§Ã£o Docker
â”œâ”€â”€ requirements.txt              # DependÃªncias Python
â”œâ”€â”€ .env.example                  # Exemplo de variÃ¡veis
â”œâ”€â”€ Makefile                      # Comandos de automaÃ§Ã£o
â””â”€â”€ README.md
```

## ğŸ’¼ Casos de Uso

### 1. Pipeline de E-commerce

**Fonte**: API de vendas, CSV de produtos  
**TransformaÃ§Ã£o**: CÃ¡lculo de mÃ©tricas (receita, itens vendidos)  
**Destino**: Tabelas fato de vendas e dimensÃµes

### 2. Weather Data Analytics

**Fonte**: OpenWeatherMap API  
**TransformaÃ§Ã£o**: AgregaÃ§Ã£o por cidade, conversÃ£o de unidades  
**Destino**: SÃ©rie temporal de clima

### 3. Customer Data Integration

**Fonte**: MÃºltiplos CSVs de diferentes sistemas  
**TransformaÃ§Ã£o**: DeduplicaÃ§Ã£o, enriquecimento  
**Destino**: Master Data de clientes

## ğŸ“Š Schema do Data Warehouse

```sql
-- Fact Tables
fact_sales (
    sale_id,
    date_id,
    customer_id,
    product_id,
    quantity,
    revenue,
    created_at
)

-- Dimension Tables
dim_date (
    date_id,
    date,
    year,
    month,
    quarter,
    day_of_week
)

dim_customer (
    customer_id,
    name,
    email,
    city,
    state,
    country
)

dim_product (
    product_id,
    name,
    category,
    price,
    cost
)
```

## ğŸ§ª Testes

```bash
# Executar todos os testes
pytest

# Com cobertura
pytest --cov=src --cov-report=html

# Testes especÃ­ficos
pytest tests/test_extract.py
```

## ğŸ“ˆ Monitoramento

- **Airflow UI**: VisualizaÃ§Ã£o de DAGs e execuÃ§Ãµes
- **Logs**: Arquivo detalhado em `data/logs/`
- **MÃ©tricas**: Tempo de execuÃ§Ã£o, linhas processadas
- **Alertas**: Email/Slack em caso de falhas

## ğŸ”§ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente (.env)

```bash
# Database
DB_HOST=localhost
DB_PORT=5432
DB_NAME=etl_warehouse
DB_USER=etl_user
DB_PASSWORD=etl_password

# API Keys
WEATHER_API_KEY=your_api_key
```

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

## ğŸ‘¤ Autor

**Seu Nome**
- GitHub: [@seu-usuario](https://github.com/seu-usuario)
- LinkedIn: [Seu Perfil](https://linkedin.com/in/seu-perfil)
- Email: seu.email@exemplo.com

## ğŸ™ Recursos

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Great Expectations](https://docs.greatexpectations.io/)

---

â­ **Se este projeto foi Ãºtil, considere dar uma estrela!**
