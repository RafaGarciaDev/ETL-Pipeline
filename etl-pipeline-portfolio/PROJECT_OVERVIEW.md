# ğŸš€ ETL Pipeline End-to-End - Project Overview

## ğŸ“‹ Para seu PortfÃ³lio

Este Ã© um projeto **completo e profissional** de ETL Pipeline demonstrando competÃªncias em:
- Engenharia de Dados
- Python avanÃ§ado
- OrquestraÃ§Ã£o com Airflow
- Design de Sistemas
- Testes e Qualidade de CÃ³digo

---

## ğŸ¯ O que este Projeto Demonstra

### 1. **Extract (ExtraÃ§Ã£o)**
âœ… ExtraÃ§Ã£o de mÃºltiplas fontes (API REST, CSV, Banco de Dados)  
âœ… Retry logic com exponential backoff  
âœ… Rate limiting e tratamento de erros  
âœ… PaginaÃ§Ã£o automÃ¡tica  
âœ… DetecÃ§Ã£o automÃ¡tica de encoding  

### 2. **Transform (TransformaÃ§Ã£o)**
âœ… Limpeza de dados (duplicatas, nulos, outliers)  
âœ… PadronizaÃ§Ã£o de colunas  
âœ… ConversÃ£o de tipos  
âœ… Enriquecimento de dados  
âœ… AgregaÃ§Ãµes complexas  
âœ… ValidaÃ§Ã£o de qualidade  

### 3. **Load (Carga)**
âœ… MÃºltiplas estratÃ©gias (append, replace, upsert)  
âœ… Batch processing para performance  
âœ… Transaction management  
âœ… CriaÃ§Ã£o automÃ¡tica de Ã­ndices  
âœ… Carga incremental  

### 4. **OrquestraÃ§Ã£o**
âœ… Apache Airflow com DAGs  
âœ… Gerenciamento de dependÃªncias  
âœ… Retry automÃ¡tico em falhas  
âœ… Logging detalhado  
âœ… NotificaÃ§Ãµes  

### 5. **Qualidade de CÃ³digo**
âœ… Testes unitÃ¡rios (pytest)  
âœ… Cobertura de cÃ³digo  
âœ… Linting (flake8, black)  
âœ… Type hints  
âœ… DocumentaÃ§Ã£o completa  

---

## ğŸ“Š Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sources   â”‚  API, CSV, DB
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Extract   â”‚  api_extractor.py, csv_extractor.py
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Transform  â”‚  transformer.py
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Load     â”‚  db_loader.py
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Warehouse  â”‚  PostgreSQL
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Airflow   â”‚  Orchestration
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—‚ï¸ Estrutura do CÃ³digo

```
etl-pipeline-portfolio/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â”œâ”€â”€ api_extractor.py      # 200+ linhas
â”‚   â”‚   â”œâ”€â”€ csv_extractor.py      # 180+ linhas
â”‚   â”‚   â””â”€â”€ db_extractor.py
â”‚   â”‚
â”‚   â”œâ”€â”€ transform/
â”‚   â”‚   â””â”€â”€ transformer.py        # 250+ linhas
â”‚   â”‚
â”‚   â”œâ”€â”€ load/
â”‚   â”‚   â””â”€â”€ db_loader.py          # 220+ linhas
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”‚
â”‚   â””â”€â”€ main.py                   # Orchestrator
â”‚
â”œâ”€â”€ airflow/dags/
â”‚   â”œâ”€â”€ etl_daily.py              # DAG principal
â”‚   â””â”€â”€ etl_weekly.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_extract.py
â”‚   â”œâ”€â”€ test_transform.py
â”‚   â””â”€â”€ test_load.py
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile                      # 30+ comandos
â””â”€â”€ requirements.txt
```

**Total: 1000+ linhas de cÃ³digo Python de produÃ§Ã£o!**

---

## ğŸ’» Stack TecnolÃ³gico

### Core
- **Python 3.11+** - Linguagem principal
- **Pandas** - ManipulaÃ§Ã£o de dados
- **SQLAlchemy** - ORM para banco de dados

### OrquestraÃ§Ã£o
- **Apache Airflow** - Workflow orchestration
- **Docker** - ContainerizaÃ§Ã£o

### Storage
- **PostgreSQL** - Data Warehouse
- **CSV/JSON** - File-based storage

### Testing & Quality
- **Pytest** - Testes unitÃ¡rios
- **Black** - Code formatting
- **Flake8** - Linting
- **Coverage** - Code coverage

---

## ğŸš€ Como Executar

### Quick Start (3 comandos)
```bash
make setup          # Setup inicial
make up             # Inicia containers
make run            # Executa pipeline
```

### Docker (Recomendado)
```bash
docker-compose up -d
# Acessar Airflow: http://localhost:8080
```

### Local
```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python src/main.py
```

---

## âœ¨ Features AvanÃ§adas

### 1. **ConfiguraÃ§Ã£o FlexÃ­vel**
- YAML configuration files
- Environment variables
- Multiple environments (dev/prod)

### 2. **Logging Completo**
- Structured logging
- Rotating file handlers
- Different log levels
- ETL-specific log methods

### 3. **Error Handling Robusto**
- Try-catch em todas as operaÃ§Ãµes
- Retry logic com exponential backoff
- Fallback strategies
- Detailed error messages

### 4. **Performance Optimization**
- Batch processing
- Chunked reading for large files
- Database connection pooling
- Incremental loads

### 5. **Data Quality**
- Schema validation
- Business rule checks
- Freshness monitoring
- Automated testing

---

## ğŸ“ˆ Casos de Uso

### 1. E-commerce Analytics
- Extrair: API de vendas + CSV de produtos
- Transformar: Calcular mÃ©tricas de receita
- Carregar: Tabelas fact de vendas

### 2. Weather Data Pipeline
- Extrair: OpenWeatherMap API
- Transformar: AgregaÃ§Ã£o por cidade
- Carregar: SÃ©rie temporal

### 3. Customer Data Integration
- Extrair: MÃºltiplos CSVs de CRM
- Transformar: DeduplicaÃ§Ã£o e enriquecimento
- Carregar: Master data de clientes

---

## ğŸ§ª Testes

```bash
# Todos os testes
make test

# Com cobertura
pytest --cov=src --cov-report=html

# Resultado esperado: >80% coverage
```

---

## ğŸ“Š MÃ©tricas do Projeto

| MÃ©trica | Valor |
|---------|-------|
| Linhas de CÃ³digo | 1000+ |
| MÃ³dulos Python | 8+ |
| Testes UnitÃ¡rios | 20+ |
| Comandos Make | 25+ |
| Cobertura de Testes | >80% |
| Airflow DAGs | 2 |
| Docker Services | 4 |

---

## ğŸ¨ Diferenciais para Recrutadores

âœ… **Production-Ready**: CÃ³digo que pode ir direto para produÃ§Ã£o  
âœ… **Best Practices**: Seguindo padrÃµes da indÃºstria  
âœ… **DocumentaÃ§Ã£o**: README, docstrings, type hints  
âœ… **Testes**: Cobertura >80%  
âœ… **DevOps**: Docker, Makefile, CI/CD  
âœ… **EscalÃ¡vel**: Modular e extensÃ­vel  

---

## ğŸ”§ Comandos Ãšteis

```bash
# Setup e execuÃ§Ã£o
make setup              # Setup inicial
make install            # Instala dependÃªncias
make up                 # Inicia containers
make run                # Executa pipeline

# Testes e qualidade
make test               # Executa testes
make lint               # Linting
make format             # Formata cÃ³digo

# Database
make db-init            # Inicializa DB
make db-shell           # Acessa DB

# Airflow
make airflow-init       # Setup Airflow
make logs               # Ver logs

# Limpeza
make clean              # Limpa artefatos
make down               # Para containers
```

---

## ğŸ“š Conceitos Demonstrados

### Design Patterns
- âœ… Factory Pattern (para extractors)
- âœ… Strategy Pattern (load strategies)
- âœ… Dependency Injection
- âœ… Separation of Concerns

### Data Engineering
- âœ… ELT vs ETL
- âœ… Incremental vs Full Load
- âœ… Data Quality Checks
- âœ… Schema Evolution

### Software Engineering
- âœ… SOLID Principles
- âœ… DRY (Don't Repeat Yourself)
- âœ… Error Handling
- âœ… Logging Best Practices

---

## ğŸ¯ Para Entrevistas

### Perguntas Frequentes

**"Conte sobre um projeto de ETL que vocÃª desenvolveu"**
> "Desenvolvi um pipeline ETL completo em Python que extrai dados de APIs REST e CSVs, aplica transformaÃ§Ãµes complexas como limpeza, deduplicaÃ§Ã£o e agregaÃ§Ãµes, e carrega em PostgreSQL. O pipeline Ã© orquestrado pelo Airflow, tem retry logic, logging detalhado e >80% de cobertura de testes."

**"Como vocÃª garante qualidade de dados?"**
> "Implemento validaÃ§Ãµes em mÃºltiplas camadas: schema validation na extraÃ§Ã£o, business rules checks na transformaÃ§Ã£o, e data quality assertions antes do load. Todos os testes sÃ£o automatizados com pytest e integrados no CI/CD."

**"Como vocÃª lida com falhas?"**
> "Uso retry logic com exponential backoff, transaction management no banco, e logging detalhado para troubleshooting. O Airflow permite retry automÃ¡tico e notificaÃ§Ãµes em caso de falha."

---

## ğŸ‘¥ PrÃ³ximos Passos

Melhorias futuras:
- [ ] IntegraÃ§Ã£o com Apache Spark para big data
- [ ] CDC (Change Data Capture)
- [ ] Data lineage tracking
- [ ] Monitoring dashboard (Grafana)
- [ ] Kubernetes deployment
- [ ] CI/CD pipeline completo

---

## ğŸ“ Contato

**[Seu Nome]**
- ğŸ“§ Email: seu.email@exemplo.com
- ğŸ’¼ LinkedIn: [seu-perfil](https://linkedin.com/in/seu-perfil)
- ğŸ™ GitHub: [@seu-usuario](https://github.com/seu-usuario)

---

**â­ Este projeto demonstra que vocÃª sabe construir pipelines de dados profissionais, escalÃ¡veis e production-ready!**
